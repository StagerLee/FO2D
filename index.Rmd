---
title: "FO2D Group: 30"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: flatly
---

```{r setup, include=FALSE}
library(flexdashboard)
```

Overview
======================================================================

Sidebar {.sidebar}
-----------------------------------------------------------------------


### *Abstract*

Musical classification schemes for grouping musical genres are fuzzy. In theory, any musical dynamic can be used to classify a musical piece to its asociated genre. Spotifys algorithm uses audio features (for the most part) to classify music, but also provides inside into its recommendation system. 

Research focus: Collect a list of songs through an onlines surbvey (50 songs should be enough?), regarding the novel phenomenon of 'situationa lusic' - ie. music used for a certain context rather than its aestethic (we could do study music??)

Visualize audio fetures and associated metada (ie. classify study music using Spotifys classification scheme); group participants by demographic (Faculty, gender, personality?) to explore how the study-music 'genre' looks for different groups. Look at patterns, similarities and differences.

How does this situational type of music (ie. study music) relate to the concept of 'genre' based on the similarities of its audio fetures?


Column {.tabset}
-----------------------------------------------------------------------

### **Audio: Corpus vs Anti-Corpus (Not Actual Data)**
The data visualized is from 2 different playlists (study music vs anti-studi music) that was pulled using the Spotify API and Python, wrangled in Rstudio (calculate mean value) and visualized using the package Plotly

```{r}
library(plotly)

fig <- plot_ly(
    type = 'scatterpolar',
    fill = 'toself'
  ) 
fig <- fig %>%
  add_trace(
    r = c(0.65, 0.71, 0.67, 0.13, 0.18, 0.08, 0.20, 0.55, 0.36),
    theta = c('Danceability','Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valance', 'Tempo'),
    name = 'Anti Corpus'
  ) 
fig <- fig %>%
  add_trace(
    r = c(0.58, 0.45, 0.12, 0.08, 0.49, 0.44, 0.15, 0.37, 0.30),
    theta = c('Danceability','Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valance', 'Tempo'),
    name = 'Corpus'
  ) 
fig <- fig %>%
  layout(
    polar = list(
      radialaxis = list(
        visible = T,
        range = c(0,1)
      )
    )
  )

fig

```

### Text Metadata
```{r}
```

Column {data-width=350}
-----------------------------------------------------------------------

### Colected / Used data as playable playlist (Test)

```{r}
```
<iframe src="https://open.spotify.com/embed/playlist/5xLSS4slREH7LE2llXT4Id" width="440" height="370" frameborder="" allowtransparency="true" allow="encrypted-media"></iframe>

### Audio Features Explenation

```{r}
```
Feature | Explanation
------------ | -------------
Valence | A measure from 0 to 1 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful), while tracks with low valence sound more negative (e.g. sad, angry).
Energy | Energy is a measure from 0 to 1 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. 
Danceability | Danceability describes how suitable a track is for dancing based on a combination of musical elements (such as tempo), measured from 0 to 1.
Instrumentalness | Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. The closer to 1, since it is measured from 0 to 1, the more instrumental the song is.
Acousticness | A confidence measure from 0 to 1 of whether the track is acoustic. 1 represents acousticness, and means there are no electric sounds involved.
Speechiness | Speechiness detects the presence of spoken words in a track. Measured from 0 to 1, the more speech, the closer to 1.
Liveness | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live, measured from 0 to 1.
Loudness | The overall loudness of a track in decibels (dB). Values typical range between -60 and 0 db (represented on a scale from 0 - 1)
Tempo | The overall estimated tempo of a track in beats per minute (BPM). Tempo is the speed or pace of a given piece and derives directly from the average beat duration.

Background {data-orientation=rows}
=====================================================================
Column
---------------------------------------------------------------------
### Introduction

- Contextualizing RQ (still have to formulate it precisely)

### Methodlogy

- Data Collection = Survey to collect songs based on a criteria (we have to fidure out what)
- Music Infromation Retrieval using Rstudio, Python and the Spoitfy API

Column
---------------------------------------------------------------------
### Survey Results

Maybe group participants by demographic and display graphs per demographic or a different criteria??


Discussion
=====================================================================

Column
---------------------------------------------------------------------
### Discussion


Column {.tabset}
---------------------------------------------------------------------
### Final Results

### Raw Data

### References